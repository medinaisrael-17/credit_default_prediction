{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb214686",
   "metadata": {},
   "source": [
    "* Problem: Predict whether a customer will default on a loan based on their financial and demographic info. \n",
    "* ML Framework: scikit-learn\n",
    "* Link to dataset: https://www.kaggle.com/datasets/uciml/default-of-credit-card-clients-dataset?resource=download "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdf1072",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=========IMPORT LIBRARIES=========\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklear.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65cef20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=========LOAD DATA=========\n",
    "#Example: local CSV\n",
    "data = pd.read_csv(\"data/UCI_Credit_Card.csv\")\n",
    "#not needed to train and test with\n",
    "data.drop(columns=['ID'])\n",
    "\n",
    "#Quick Look\n",
    "data.head()\n",
    "data.info()\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056fae2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=========PREPROCESSING=========\n",
    "# Handle missing values        \n",
    "# Encode Categorical features  \n",
    "# Scale numerical features     \n",
    "\n",
    "#Example of preprocessing pipeline\n",
    "categorical_features = ['SEX', 'EDUCATION', 'MARRIAGE']\n",
    "numeric_features = [col for col in data.columns if col not in categorical_features + ['default.payment.next.month']]\n",
    "\n",
    "# preprocessor - a preprocessor is basically a *set of instructions that tells the model how to handle \n",
    "# raw data before training*. Real-world data is not usually ready to go straight into machine learning model. \n",
    "# It is the data cleaner + translator that makes messy raw data usable for ML.\n",
    "# for ex: \n",
    "#   * some numbers might be on different scales (think like income in thousands, but then age in years, credit score in 3 dig.)\n",
    "#   * some features are categorical like martial status which a model cant read as text\n",
    "#   * some columns might have missing values\n",
    "# \n",
    "# to solve this the preprocessor\n",
    "#   * scales numeric features -> so features are comparable in test_size\n",
    "#   * encodes categorical features -> turning words into numbers\n",
    "#   * handle missing data, feature selection, etc.\n",
    "\n",
    "\n",
    "# StandardScaler is applied to numeric columns which basically means making them 0, std 1.\n",
    "# OneHotEncoder is applied to categorical columns which converts text to binary columns.\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(), categorical_features)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4b4c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=========SPLIT DATA=========\n",
    "\n",
    "#This part does literally what it says:\n",
    "#Drop 'Defaulted' from the data set\n",
    "X = data.drop(columns=['default.payment.next.month'])\n",
    "#'y' will only extrapolate the target column 'default.payment.next.month'\n",
    "y = data['default.payment.next.month']\n",
    "\n",
    "#The train_test_split function splits the dataset into two parts\n",
    "#    * Training set (what the model learns from)\n",
    "#    * Test set (unseen data to check if the model generalizes)\n",
    "#\n",
    "#Parameters:\n",
    "#   * X,y -> input features and target\n",
    "#   * test_size = 0.2 -> 20% of the data goes to test, 80% to train\n",
    "#   * random_state = 42 -> ensures the split is the same every time (reproducibility)\n",
    "#\n",
    "#Result\n",
    "#   * X_train = training input features\n",
    "#   * X_test = test input features\n",
    "#   * y_train = training target values\n",
    "#   * y_test = test target values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbaf1b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=========BUILD MODEL=========\n",
    "\n",
    "#Random Forest Classifier inside a pipeline\n",
    "\n",
    "#What random_state does:\n",
    "#   * random_state is a seed for the random number generator used by functions like train_test_split,\n",
    "#     shuffle, or any algorithim that has a randomness (like RandomForest or KMeans).\n",
    "#   * Setting it ensures reproducibility: every time the code gets ran with the same random_state,\n",
    "#     it ensure the same split or results. W/o it, each run could give slightly different outputs. \n",
    "#     Using 42 ensures the generator starts at the same point every time. \"Randomly picking rows the same way\n",
    "#     each run.      \n",
    "#\n",
    "#RandomForestClassifier is a type of ensemble machine learning module used for classification tasks\n",
    "#like predicting discrete categories such as yes/no, 0/1, etc.\n",
    "#\"forest\" meaning lots of decision trees.\n",
    "#\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                        ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))])\n",
    "\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed46ac9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=========EVALUATE MODEL=========\n",
    "\n",
    "#predict() basically \n",
    "#   * takes each test row\n",
    "#   * applies the learned weight and rules from earlier\n",
    "#   * produces a probability\n",
    "#   * converts probability -> 0 or 1 (threshold = 0.5 by default)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fdc4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=========SAVE MODEL=========\n",
    "\n",
    "import joblib\n",
    "joblib.dump(clf, 'models/credit_default_model.pkl')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
